{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "935ab4e9-a045-409d-b9ba-44741f52ebd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/harvard-visionlab/lrm-steering/zipball/main\" to /n/alvarez_lab_tier1/Lab/cache/torch/hub/main.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading weights for alexnet_lrm3, hash_id=63ab1b3b06\n",
      "https://s3.us-east-1.wasabisys.com/visionlab-projects/dnn_feedback_dev/logs/set15/set15_alexnet_torchvision_imagenet1k_lrm_3back_2steps/28453e80-c5e5-4d76-bc81-99c5fade39ff/set15_alexnet_torchvision_imagenet1k_lrm_3back_2steps_final_weights-63ab1b3b06.pth\n",
      "local_filename: /n/alvarez_lab_tier1/Lab/cache/torch/hub/set15_alexnet_torchvision_imagenet1k_lrm_3back_2steps_final_weights-63ab1b3b06.pth\n",
      "<All keys matched successfully>\n",
      "LRMNet(\n",
      "  (feedforward): AlexNet(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "      (1): ReLU()\n",
      "      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (4): ReLU()\n",
      "      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU()\n",
      "      (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (9): ReLU()\n",
      "      (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU()\n",
      "      (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "    (classifier): Sequential(\n",
      "      (0): Dropout(p=0.5, inplace=False)\n",
      "      (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "      (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (lrm): Sequential(\n",
      "    (features_8_modulation): LongRangeModulation(\n",
      "      (from_classifier_6_to_features_8): ModBlock(\n",
      "        (rescale): NormSquashResize(\n",
      "          (norm): ChannelNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
      "          (squash): FeedbackScale(mode='tanh')\n",
      "          (interp): AddSpatialDimension()\n",
      "        )\n",
      "        (modulation): Conv2d(1000, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (pre_mod_output): Identity()\n",
      "      (total_mod): Identity()\n",
      "      (post_mod_output): Identity()\n",
      "    )\n",
      "    (features_0_modulation): LongRangeModulation(\n",
      "      (from_features_9_to_features_0): ModBlock(\n",
      "        (rescale): NormSquashResize(\n",
      "          (norm): AdaptiveFullstackNorm((256, 55, 55), eps=1e-05, elementwise_affine=True)\n",
      "          (squash): FeedbackScale(mode='tanh')\n",
      "          (interp): AdaptiveUpsample(upsample_mode='UpsampleBilinear')\n",
      "        )\n",
      "        (modulation): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (pre_mod_output): Identity()\n",
      "      (total_mod): Identity()\n",
      "      (post_mod_output): Identity()\n",
      "    )\n",
      "    (features_10_modulation): LongRangeModulation(\n",
      "      (from_classifier_6_to_features_10): ModBlock(\n",
      "        (rescale): NormSquashResize(\n",
      "          (norm): ChannelNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
      "          (squash): FeedbackScale(mode='tanh')\n",
      "          (interp): AddSpatialDimension()\n",
      "        )\n",
      "        (modulation): Conv2d(1000, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (pre_mod_output): Identity()\n",
      "      (total_mod): Identity()\n",
      "      (post_mod_output): Identity()\n",
      "    )\n",
      "    (features_3_modulation): LongRangeModulation(\n",
      "      (from_features_12_to_features_3): ModBlock(\n",
      "        (rescale): NormSquashResize(\n",
      "          (norm): AdaptiveFullstackNorm((256, 27, 27), eps=1e-05, elementwise_affine=True)\n",
      "          (squash): FeedbackScale(mode='tanh')\n",
      "          (interp): AdaptiveUpsample(upsample_mode='UpsampleBilinear')\n",
      "        )\n",
      "        (modulation): Conv2d(256, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (pre_mod_output): Identity()\n",
      "      (total_mod): Identity()\n",
      "      (post_mod_output): Identity()\n",
      "    )\n",
      "    (features_6_modulation): LongRangeModulation(\n",
      "      (from_classifier_2_to_features_6): ModBlock(\n",
      "        (rescale): NormSquashResize(\n",
      "          (norm): ChannelNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (squash): FeedbackScale(mode='tanh')\n",
      "          (interp): AddSpatialDimension()\n",
      "        )\n",
      "        (modulation): Conv2d(4096, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (pre_mod_output): Identity()\n",
      "      (total_mod): Identity()\n",
      "      (post_mod_output): Identity()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model, transforms = torch.hub.load('harvard-visionlab/lrm-steering', 'alexnet_lrm3', pretrained=True, steering=False, force_reload=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9192750-1706-4175-865c-ba10286229a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model.eval()\n",
    "x = torch.rand(10,3,224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b12f3d10-8089-4e99-93ac-86fadb9d2cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2403,  0.6444,  2.1112,  ..., -0.4410, -0.4032, -0.4524],\n",
       "        [-1.0969,  0.4178,  1.8896,  ..., -0.4534, -0.7122, -0.8312],\n",
       "        [-1.1433,  0.5999,  2.0798,  ..., -0.4515, -0.6131, -0.8496],\n",
       "        ...,\n",
       "        [-1.1844,  0.5734,  1.9856,  ..., -0.3310, -0.1186, -0.7900],\n",
       "        [-0.9280,  0.5157,  2.1720,  ..., -0.3788, -0.5476, -0.8095],\n",
       "        [-1.2469,  0.5101,  1.9878,  ..., -0.4985, -0.5355, -0.3992]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(x, forward_passes=1)\n",
    "len(out)   \n",
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faf807ee-7790-461d-ae11-c218aee68d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2403,  0.6444,  2.1112,  ..., -0.4410, -0.4032, -0.4524],\n",
       "        [-1.0969,  0.4178,  1.8896,  ..., -0.4534, -0.7122, -0.8312],\n",
       "        [-1.1433,  0.5999,  2.0798,  ..., -0.4515, -0.6131, -0.8496],\n",
       "        ...,\n",
       "        [-1.1844,  0.5734,  1.9856,  ..., -0.3310, -0.1186, -0.7900],\n",
       "        [-0.9280,  0.5157,  2.1720,  ..., -0.3788, -0.5476, -0.8095],\n",
       "        [-1.2469,  0.5101,  1.9878,  ..., -0.4985, -0.5355, -0.3992]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(x, forward_passes=1)\n",
    "len(out)   \n",
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4d3c26c-ee19-4826-823a-b69eb18521fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2403,  0.6444,  2.1112,  ..., -0.4410, -0.4032, -0.4524],\n",
       "        [-1.0969,  0.4178,  1.8896,  ..., -0.4534, -0.7122, -0.8312],\n",
       "        [-1.1433,  0.5999,  2.0798,  ..., -0.4515, -0.6131, -0.8496],\n",
       "        ...,\n",
       "        [-1.1844,  0.5734,  1.9856,  ..., -0.3310, -0.1186, -0.7900],\n",
       "        [-0.9280,  0.5157,  2.1720,  ..., -0.3788, -0.5476, -0.8095],\n",
       "        [-1.2469,  0.5101,  1.9878,  ..., -0.4985, -0.5355, -0.3992]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(x, forward_passes=2)\n",
    "len(out)   \n",
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9400577f-8c0f-47ac-986f-d18c24bcb953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7497,  0.3781,  2.0520,  ..., -0.1933, -1.2317, -0.6173],\n",
       "        [-0.3226,  0.3404,  1.8778,  ..., -0.1909, -1.5044, -0.6404],\n",
       "        [-0.6053,  0.4258,  1.9751,  ...,  0.0723, -1.2615, -0.8122],\n",
       "        ...,\n",
       "        [-0.6668,  0.3714,  1.9055,  ...,  0.2147, -1.0289, -0.8298],\n",
       "        [-0.5214,  0.2333,  1.9206,  ...,  0.0031, -1.2414, -0.7742],\n",
       "        [-0.7068,  0.3681,  1.9491,  ...,  0.0496, -1.1576, -0.7089]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "066f648b-d9af-47fa-be0a-ab6f5694bc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ModelWrapper(nn.Module):\n",
    "    def __init__(self, model, forward_passes=2):\n",
    "        super(ModelWrapper, self).__init__() \n",
    "        self.model = model\n",
    "        self.forward_passes = forward_passes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.model(x, forward_passes=self.forward_passes)\n",
    "        print(len(out))\n",
    "        return out[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66deb73f-8e91-4a8e-bd80-79f2884c2a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /n/alvarez_lab_tier1/Lab/cache/torch/hub/harvard-visionlab_lrm-steering_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading weights for alexnet_lrm3, hash_id=63ab1b3b06\n",
      "https://s3.us-east-1.wasabisys.com/visionlab-projects/dnn_feedback_dev/logs/set15/set15_alexnet_torchvision_imagenet1k_lrm_3back_2steps/28453e80-c5e5-4d76-bc81-99c5fade39ff/set15_alexnet_torchvision_imagenet1k_lrm_3back_2steps_final_weights-63ab1b3b06.pth\n",
      "local_filename: /n/alvarez_lab_tier1/Lab/cache/torch/hub/set15_alexnet_torchvision_imagenet1k_lrm_3back_2steps_final_weights-63ab1b3b06.pth\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModelWrapper(\n",
       "  (model): LRMNet(\n",
       "    (feedforward): AlexNet(\n",
       "      (features): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "        (1): ReLU()\n",
       "        (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "        (4): ReLU()\n",
       "        (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): ReLU()\n",
       "        (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (9): ReLU()\n",
       "        (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (11): ReLU()\n",
       "        (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "      (classifier): Sequential(\n",
       "        (0): Dropout(p=0.5, inplace=False)\n",
       "        (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Dropout(p=0.5, inplace=False)\n",
       "        (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (lrm): Sequential(\n",
       "      (features_8_modulation): LongRangeModulation(\n",
       "        (from_classifier_6_to_features_8): ModBlock(\n",
       "          (rescale): NormSquashResize(\n",
       "            (norm): ChannelNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
       "            (squash): FeedbackScale(mode='tanh')\n",
       "            (interp): AddSpatialDimension()\n",
       "          )\n",
       "          (modulation): Conv2d(1000, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (pre_mod_output): Identity()\n",
       "        (total_mod): Identity()\n",
       "        (post_mod_output): Identity()\n",
       "      )\n",
       "      (features_0_modulation): LongRangeModulation(\n",
       "        (from_features_9_to_features_0): ModBlock(\n",
       "          (rescale): NormSquashResize(\n",
       "            (norm): AdaptiveFullstackNorm((256, 55, 55), eps=1e-05, elementwise_affine=True)\n",
       "            (squash): FeedbackScale(mode='tanh')\n",
       "            (interp): AdaptiveUpsample(upsample_mode='UpsampleBilinear')\n",
       "          )\n",
       "          (modulation): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (pre_mod_output): Identity()\n",
       "        (total_mod): Identity()\n",
       "        (post_mod_output): Identity()\n",
       "      )\n",
       "      (features_10_modulation): LongRangeModulation(\n",
       "        (from_classifier_6_to_features_10): ModBlock(\n",
       "          (rescale): NormSquashResize(\n",
       "            (norm): ChannelNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
       "            (squash): FeedbackScale(mode='tanh')\n",
       "            (interp): AddSpatialDimension()\n",
       "          )\n",
       "          (modulation): Conv2d(1000, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (pre_mod_output): Identity()\n",
       "        (total_mod): Identity()\n",
       "        (post_mod_output): Identity()\n",
       "      )\n",
       "      (features_3_modulation): LongRangeModulation(\n",
       "        (from_features_12_to_features_3): ModBlock(\n",
       "          (rescale): NormSquashResize(\n",
       "            (norm): AdaptiveFullstackNorm((256, 27, 27), eps=1e-05, elementwise_affine=True)\n",
       "            (squash): FeedbackScale(mode='tanh')\n",
       "            (interp): AdaptiveUpsample(upsample_mode='UpsampleBilinear')\n",
       "          )\n",
       "          (modulation): Conv2d(256, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (pre_mod_output): Identity()\n",
       "        (total_mod): Identity()\n",
       "        (post_mod_output): Identity()\n",
       "      )\n",
       "      (features_6_modulation): LongRangeModulation(\n",
       "        (from_classifier_2_to_features_6): ModBlock(\n",
       "          (rescale): NormSquashResize(\n",
       "            (norm): ChannelNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (squash): FeedbackScale(mode='tanh')\n",
       "            (interp): AddSpatialDimension()\n",
       "          )\n",
       "          (modulation): Conv2d(4096, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (pre_mod_output): Identity()\n",
       "        (total_mod): Identity()\n",
       "        (post_mod_output): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrm, transforms = torch.hub.load('harvard-visionlab/lrm-steering', 'alexnet_lrm3', pretrained=True, steering=False, force_reload=False)\n",
    "model = ModelWrapper(lrm, forward_passes=2)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75201d56-235b-4c3e-8f2f-b451abccea2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1000])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(10, 3, 224, 224)\n",
    "with torch.no_grad():\n",
    "    out = model(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099476d3-5aaa-42aa-b4f6-bf2dbf9f3c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainscore",
   "language": "python",
   "name": "brainscore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
